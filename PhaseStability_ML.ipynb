{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062d4b72",
   "metadata": {},
   "source": [
    "# Liquid Formulations Stability Classifier\n",
    "## Aniket Chitre, University of Cambridge\n",
    "\n",
    "This notebook is used to train a phase stability classifier over an experimental formulations dataset to guide which samples to prepare in the next DoE (design of experiments) step. The notebook reads several CSV files generated via a sister DoE code in R:\n",
    "\n",
    "* `ExperimentalSet.csv` the pre-processed experimental dataset where the formulations are represented by the scaled  concentrations of the surfactants' functional groups and the polymer & thickener concentrations.\n",
    "* `CandidateSet.csv` the full candidate list of experiments in the same pre-processed format as the experimental data.\n",
    "* `CandidateDesign.csv` the full candidate list of experiments in the format of the MaxProQQ (DoE package) design variables.\n",
    "\n",
    "The code trains and tunes several machine learning models looking for the best phase stability classifier for the given formulations problem. This is then used to predict the stability of the candidate points and restrict the full candidate set to a more suitable sub-set of potential experiments. These are exported in the `RestrictedCandDesign.csv` file which is read in the DoE programme written in R, to ultimately generate the next suggested set of experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fbfe2",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 600\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "from IPython.display import Image\n",
    "import re \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c825b6",
   "metadata": {},
   "source": [
    "# 1. Load Experimental Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c971946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ExperimentalSet/Expt_P1_T1.csv', index_col=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_P1T1 = pd.read_csv('ExperimentalSet/Expt_P1_T1.csv', index_col=[0])\n",
    "# x_P1T2 = pd.read_csv('ExperimentalSet/Expt_P1_T2.csv', index_col=[0])\n",
    "# x_P4T1 = pd.read_csv('ExperimentalSet/Expt_P4_T1.csv', index_col=[0])\n",
    "\n",
    "# X_P1T1 = x_P1T1.iloc[:, :-1]\n",
    "# X_P1T2 = x_P1T2.iloc[:, :-1]\n",
    "# X_P4T1 = x_P4T1.iloc[:, :-1]\n",
    "\n",
    "# y_P1T1 = x_P1T1.iloc[:, -1]\n",
    "# y_P1T2 = x_P1T2.iloc[:, -1]\n",
    "# y_P4T1 = x_P4T1.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into input variables (features) & output\n",
    "# last column of the df is the classification target = phase stability\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cace931",
   "metadata": {},
   "source": [
    "# 2. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11549fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train: test split due to the data imbalance: majority unstable formulations\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "# X_train_P1T1, X_test_P1T1, y_train_P1T1, y_test_P1T1 = train_test_split(X_P1T1, y_P1T1, stratify=y_P1T1, test_size=0.25, random_state=42)\n",
    "# X_train_P1T2, X_test_P1T2, y_train_P1T2, y_test_P1T2 = train_test_split(X_P1T2, y_P1T2, stratify=y_P1T2, test_size=0.25, random_state=42)\n",
    "# X_train_P4T1, X_test_P4T1, y_train_P4T1, y_test_P4T1 = train_test_split(X_P4T1, y_P4T1, stratify=y_P4T1, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2794e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "print(y_train.value_counts())\n",
    "print('\\n')\n",
    "print('Test set:')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cd7ca",
   "metadata": {},
   "source": [
    "*Key: True = stable, False = unstable*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c4df",
   "metadata": {},
   "source": [
    "## 2.1 Shortlisting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5150a86",
   "metadata": {},
   "source": [
    "### Setting a Baseline\n",
    "\n",
    "Before we delve into tuning more complex models, it is often useful to train some common model types with their default hyperparameters to set a baseline with which to compare the more carefully selected and tuned models.\n",
    "\n",
    "We will begin looking at logistic regression, Naive Bayes and decision tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88681d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call the models \n",
    "logreg_clf = LogisticRegression()\n",
    "nb_model = GaussianNB()\n",
    "dtc_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#fit the models \n",
    "logreg_clf.fit(X_train, y_train)\n",
    "nb_model.fit(X_train, y_train)\n",
    "dtc_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test ste\n",
    "logreg_pred = logreg_clf.predict(X_test)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "dtc_pred = dtc_model.predict(X_test)\n",
    "\n",
    "# confusion_matrix(y_true, y_pred) - it's essential to get the order of the arguments correct! \n",
    "\n",
    "print('Logistic Regression')\n",
    "print(confusion_matrix(y_test, logreg_pred))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Naive Bayes')\n",
    "print(confusion_matrix(y_test, nb_pred))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Decision Tree')\n",
    "print(confusion_matrix(y_test, dtc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16102be6",
   "metadata": {},
   "source": [
    "The quickest way for me to assess the performance of the models is to look at the results on the test set presented in the form of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ImagesforJupyter/ConfusionMatrix.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b2209",
   "metadata": {},
   "source": [
    "The above models are only used to provide a baseline, as they have some known shortcomings and there are likely to be better models available:\n",
    "\n",
    "* Logistic regression is a type of generalised linear model, however, the formulation's design space is non-linear and highly complex.\n",
    "* Similarly, there are issues with the Naive Bayes (NB) model, as there are lots of zero counts for functional groups. It's known that the NB model struggles with this (e.g., https://www.atoti.io/articles/how-to-solve-the-zero-frequency-problem-in-naive-bayes)\n",
    "* And decision trees typically overfit as the structure of a tree can be highly dependent on the training data. \n",
    "\n",
    "The most promising models to test and tune are random forests and gradient boosted algorithms, which are both ensembles of decision trees and typically perform very well on structured tabular data, like this liquid formulations problem. Additionally, support vector machines are another method to test more rigourously. More expensive ML methods, like neural networks, are being avoided, as there is not enough data relative to the number of paramteres to train such an architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42) # random forest with its default parameters\n",
    "\n",
    "svc = SVC(random_state=42) # Support Vector machine with its default parameters \n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=42) # Gradient Boosting classifier with its default parameters\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "rf_pred  = rf_clf.predict(X_test)\n",
    "\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "gbc_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302ab92",
   "metadata": {},
   "source": [
    "*N.b.* See this article (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2264-5) which suggests random forests are the new \"default choice\" taking over from logistic regression. This is also commonly documented in the online literature from several machine learning blogs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaee002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results presented as confusion matrix')\n",
    "print('\\n')\n",
    "\n",
    "print('Random Forest')\n",
    "print(confusion_matrix(y_test, rf_pred))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Support Vector Machine')\n",
    "print(confusion_matrix(y_test, svc_pred,))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Gradient Boosting Classifier')\n",
    "print(confusion_matrix(y_test, gbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, gbc_pred),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, gbc_pred, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c743a12",
   "metadata": {},
   "source": [
    "## 2.2 Tuning Candidate Models\n",
    "\n",
    "There are three main models which I am interested in training and tuning and then I will select the best performing one to develop the restricted candidate set of experiments for this model-based DoE.\n",
    "\n",
    "1. Random Forests \n",
    "2. Support Vector Machines\n",
    "3. Xgboost - Gradient Boosting\n",
    "\n",
    "Models #1 and 3 have a particularly nice feature that we can intriniscally provide an estimate for feature importance and the surfactants in the formulation have purposefully been represented as concentrations of their functional groups to give the results a level of chemical interpretability.\n",
    "\n",
    "We will test out three different methods of hyperparameter tuning:\n",
    "\n",
    "1. Use of **GridSearchCV** to try out all possible, user-defined, combinations of hyperparameters.\n",
    "2. Use of **RandomSearchCV** to try out possible permutations of hyperparameters sampled from a defined search sapce.\n",
    "3. **Bayesian optimisation** (BO) to search for the optimal hyperparameters guided through an informed learning approach.\n",
    "\n",
    "The `bayes_opt` package (https://github.com/fmfn/BayesianOptimization) is used for the BO and several tutorials can be found online how to apply BO for hyperparmeter tuning: https://www.analyticsvidhya.com/blog/2021/05/bayesian-optimization-bayes_opt-or-hyperopt/\n",
    "\n",
    "In both cases the **weighted F1 score** will be used as the evaluation metric. Particularly for an imbalanced dataset like this it's important not to look at accuracy.\n",
    "\n",
    "The F1 score is defined as the harmonic mean between precision and recall \n",
    "\n",
    "\n",
    "$$ F_1 = \\frac{2 \\cdot precision \\cdot recall}{(precision + recall)} $$\n",
    "\n",
    "where $precision = \\frac{TP}{TP + FP} $ and $ recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Recall is also known as sensitivity or the true positive rate. And if you take the F1 scores for the TP and TN clases and weight them by the proportion in each of the False and True classes, then we get the weighted F1 score. This is a suitable evaluation metric for this dataset. **The best model will be selected as the model with the highest weighted F1 score when evaluated on the held-out test set.**\n",
    "\n",
    "When training the models, **k-folds cross validation with k=6 will be used**. An illustration is provided below (where k = 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ImagesforJupyter/kFoldsCV.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae63829",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_n = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbce1d3",
   "metadata": {},
   "source": [
    "### 2.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518592f",
   "metadata": {},
   "source": [
    "#### a) GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Model documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "rf_params = dict(\n",
    "    n_estimators = [5, 10, 15, 20, 50, 75, 100], # number of trees in the forest \n",
    "    criterion = [\"gini\"], # default criterion & the more efficient metric vs. entropy\n",
    "    max_depth = [None], # default = None, exapnds nodes until all leaves are pure or until all leaves contain less than min_samples_split\n",
    "    min_samples_split = [2, 3, 4, 5], # the minimum number of samples required to split an internal node \n",
    "    min_samples_leaf = [1, 2, 3, 4], # the minimum number of samples to be at a leaf node - this & above help prevent overfitting - trees don't have to go down to always producing pure nodes \n",
    "    max_features = [0.25, 0.5, 0.75, None], # the number of features to consider when looking at the best split - not the total number features the model considers! \n",
    "    class_weight=[{False: 1, True: 1},\n",
    "                  {False: 1, True: 2}] # investigate the effect of biasing the model to the true class    \n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1) # n_jobs = -1 instructs the computer to use all available CPU power training the model \n",
    "\n",
    "rf_gcv = GridSearchCV(estimator=rf, param_grid=rf_params, cv=cv_n, n_jobs=-1, verbose=1, scoring='f1_weighted')\n",
    "\n",
    "rf_gcv.fit(X_train, y_train)\n",
    "\n",
    "print('It takes %s seconds' % round((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model accuracy: {round(rf_gcv.best_score_,3)}\")\n",
    "\n",
    "print(f\"Best hyperparamters: {rf_gcv.best_params_}\")\n",
    "\n",
    "\n",
    "rf_GridSearch_best_model = rf_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fe6a7",
   "metadata": {},
   "source": [
    "#### Evaluate Results \n",
    "\n",
    "Use the performance on the train set vs. test set to gauge if the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204132d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_BestGridSearch = rf_GridSearch_best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_rf_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f035d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_rf_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9880b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rf_BestGridSearch = rf_GridSearch_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10169f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_rf_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_rf_BestGridSearch),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_rf_BestGridSearch, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_rf_BestGridSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d872737",
   "metadata": {},
   "source": [
    "#### b) RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [x for x in np.linspace(start=0.2, stop=1.0, num=10)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 6 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 400, cv=cv_n, verbose=1, random_state=42, n_jobs = -1, scoring='f1_weighted')\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4873919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model accuracy: {round(rf_random.best_score_,3)}\")\n",
    "\n",
    "print(f\"Best hyperparamters: {rf_random.best_params_}\")\n",
    "\n",
    "\n",
    "rf_RandomSearch_best_model = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_BestRandomSearch = rf_RandomSearch_best_model.predict(X_train)\n",
    "y_test_rf_BestRandomSearch = rf_RandomSearch_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75996f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_rf_BestRandomSearch),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_rf_BestRandomSearch, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414432e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_rf_BestRandomSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_rf_BestRandomSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5a52e",
   "metadata": {},
   "source": [
    "#### c) Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_clf_bo(n_estimators, min_samples_split, min_samples_leaf, max_features):\n",
    "    \n",
    "    params_rf = {}\n",
    "    params_rf['n_estimators'] = round(n_estimators) # round so only tries integer values of n_estimators \n",
    "    params_rf['min_samples_split'] = round(min_samples_split)\n",
    "    params_rf['min_samples_leaf'] = round(min_samples_leaf)\n",
    "    params_rf['max_features'] = max_features\n",
    "    \n",
    "    scores = cross_val_score(RandomForestClassifier(random_state=42, n_jobs=-1, **params_rf),\n",
    "                             X_train, y_train, scoring='f1_weighted', cv=cv_n).mean() # tried lots of different score metrics, yet GridSeach CV result above outperforms BO performance on test class \n",
    "    score = scores.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fa321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "params_rf = {\n",
    "    'n_estimators': (5, 100),\n",
    "    'min_samples_split': (2,10),\n",
    "    'min_samples_leaf': (2,10),\n",
    "    'max_features': (0.25,1.0)\n",
    "}\n",
    "\n",
    "rf_bo = BayesianOptimization(rf_clf_bo, params_rf, random_state=42)\n",
    "rf_bo.maximize(init_points=400, n_iter=100) # starts with 100 random points & goes for 400 iterations \n",
    "print('It takes %s seconds' % round((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae244ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_BO_rf = rf_bo.max['params']\n",
    "params_BO_rf['min_samples_leaf'] = round(params_BO_rf['min_samples_leaf']) \n",
    "params_BO_rf['min_samples_split'] = round(params_BO_rf['min_samples_split'])\n",
    "params_BO_rf['n_estimators'] = round(params_BO_rf['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_BO_best_model = RandomForestClassifier(**params_BO_rf, random_state=42)\n",
    "rf_BO_best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_BestBO = rf_BO_best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ba218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_rf_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_rf_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb50616",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rf_BestBO = rf_BO_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ac4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_rf_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ebb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_rf_BestBO),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_rf_BestBO, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7057db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_rf_BestBO))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fffd9",
   "metadata": {},
   "source": [
    "### Model Interpretability\n",
    "\n",
    "One of the advantages of tree-based models is that they have an in-built method for looking at the feature importances. Similarly, the SHAP library (https://github.com/slundberg/shap) can also be used to look at the contribution of features to the final predictions.\n",
    "\n",
    "Before looking into these methods it is first essential to understand that **feature importances/SHAP values can only be relied on if the model itself is accurate (low bias, low variance)**.\n",
    "\n",
    "There is also a key difference between feature importances and SHAP values. This post provides a good explanation: (https://datascience.stackexchange.com/questions/99650/difference-between-feature-effect-and-feature-importance) \n",
    "\n",
    "*\"The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. [...]\" SHAP feature importance is an alternative to permutation feature importance. There is a big difference between both importance measures: permutation feature importance is based on the decrease in model performance. SHAP is based on the magnitude of feature attributions.*\n",
    "\n",
    "This is a nice blog post for different ways to compute feature importance with RFs: https://mljar.com/blog/feature-importance-in-random-forest/\n",
    "\n",
    "See the following tutorial on **SHAP feature explanations**: https://www.youtube.com/watch?v=9haIOplEIGM&ab_channel=DeepFindr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c1597",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc14327",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features_sorted_idx = best_model.feature_importances_.argsort()\n",
    "plt.barh(X.columns[rf_features_sorted_idx], best_model.feature_importances_[rf_features_sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\", fontsize=14);\n",
    "plt.xticks(fontsize=14);\n",
    "plt.yticks(fontsize=14);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974967a",
   "metadata": {},
   "source": [
    "#### SHAP Feature Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_explainer = shap.TreeExplainer(best_model) \n",
    "rf_shap_values = rf_explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8037e",
   "metadata": {},
   "source": [
    "With the random forest model, we will get SHAP values for all of the classes, in this binary classification, for both the unstable (False) and stable (True) samples, hence below we want to look at: \n",
    "\n",
    "`rf_shap_values[instance,:,class]` where `instance` refers to the sample and `class=1` will be the stable class and we will want to look at `:` all the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(rf_shap_values[4,:,1]) # binary output - 0 = False class, 1 = True class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80071108",
   "metadata": {},
   "source": [
    "Here we are just looking at samples one-by-one, a full summary of feature importances can also be visualised with the `beeswarm()` plot, however, this is only looked at for the best selected model below in Section 2.3.\n",
    "\n",
    "What we see from this plot is:\n",
    "\n",
    "* `base value` is the `mean(model.predict(X))` - given we didn't know any of the features for the particular sample, we would guess the probability of the output to be the mean of the full set - overall probability of a stable sample.\n",
    "* Then we look at how the features of the particular sample actually drive the prediction to `f(x)` with the red features increasing the likelihood of stability in this scenario & blue decreasing the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91b5c0",
   "metadata": {},
   "source": [
    "### 2.2.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Model documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "svc_params = dict(\n",
    "    C = [2, 4, 8, 10, 20], # regularisation parameter - strength of regularisation is inversely proportional to C \n",
    "    kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \n",
    "    degree = [2, 3, 4, 5], # degree of polynomal, ignored if other kernels selected\n",
    "    class_weight=[{False: 1, True: 1},\n",
    "                  {False: 1, True: 2}] # investigate the effect of biasing the model to the true class    \n",
    ")\n",
    "\n",
    "\n",
    "svc = SVC(random_state=42, probability=True)\n",
    "\n",
    "svc_gcv = GridSearchCV(estimator=svc, param_grid=svc_params, cv=cv_n, n_jobs=-1, verbose=1, scoring='f1_weighted')\n",
    "\n",
    "svc_gcv.fit(X_train, y_train)\n",
    "\n",
    "print('It takes %s seconds' % round((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model accuracy: {round(svc_gcv.best_score_,3)}\")\n",
    "\n",
    "print(f\"Best hyperparamters: {svc_gcv.best_params_}\")\n",
    "\n",
    "\n",
    "svc_GridSearch_best_model = svc_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aab747",
   "metadata": {},
   "source": [
    "#### Evaluate Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf857a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_svc_BestGridSearch = svc_GridSearch_best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf06528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_svc_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_svc_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4577b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_svc_BestGridSearch = svc_GridSearch_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e86589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_svc_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_svc_BestGridSearch),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_svc_BestGridSearch, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_svc_BestGridSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f5bf6",
   "metadata": {},
   "source": [
    "### 2.2.3 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ff958",
   "metadata": {},
   "source": [
    "With XGBoost the square brackets which are in the SMILES fragments cause a problem with the feature names as this algorithm cannot accept this input. https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n",
    "\n",
    "Therefore, regex operations are used in the following code block to substitute the `[` or `]` for `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc002292",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# copy the feature sets as don't want to overwrite these directly\n",
    "X_train_xgb = X_train.copy()\n",
    "X_test_xgb  = X_test.copy()\n",
    "\n",
    "X_train_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']'))) else col for col in X_train_xgb.columns.values]\n",
    "X_test_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']'))) else col for col in X_test_xgb.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ae109",
   "metadata": {},
   "source": [
    "#### a) GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb835b0b",
   "metadata": {},
   "source": [
    "The following two blog posts provide a great explanation of the hyperparameters available to be tuned with XGBoost:\n",
    "\n",
    "1. https://neptune.ai/blog/xgboost-vs-lightgbm\n",
    "2. https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook\n",
    "\n",
    "Josh Starmer's StatQuest provides a great explanation of how XGBoost works for classification: https://www.youtube.com/watch?v=8b1JEDvenQU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd80c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgb_params = dict(\n",
    "    learning_rate = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5], # lower learning rate makes the model more conservative - also increases training time \n",
    "    n_estimators = [10, 20, 50, 100, 150], # number of weak learners - decision trees \n",
    "    max_depth = [3, 4, 5, 6, 7],  # maximum depth of a tree - lower value will result in less overfitting\n",
    "    min_child_weight = [2, 4, 8], # higher values used to control overfitting\n",
    "    gamma = [0, 0.1, 0.5, 1, 10], # gamma specifies the minimum loss reduction required to make a split\n",
    "    colsample_bytree = [0.5, 0.75, 0.95] # samples a fraction of the columns when constructing each tree - again, to prevent overfitting\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42, n_jobs=-1) # n_jobs = -1 instructs the computer to use all available CPU power training the model \n",
    "\n",
    "xgb_gcv = GridSearchCV(estimator=xgb, param_grid=xgb_params, cv=cv_n, n_jobs=-1, verbose=1, scoring='f1_weighted')\n",
    "\n",
    "xgb_gcv.fit(X_train_xgb, y_train)\n",
    "\n",
    "print('It takes %s seconds' % round((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef19d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model accuracy: {round(xgb_gcv.best_score_,3)}\")\n",
    "\n",
    "print(f\"Best hyperparamters: {xgb_gcv.best_params_}\")\n",
    "\n",
    "\n",
    "xgb_GridSearch_best_model = xgb_gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ec3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_xgb_BestGridSearch = xgb_GridSearch_best_model.predict(X_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d00d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_xgb_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_xgb_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cad544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_xgb_BestGridSearch = xgb_GridSearch_best_model.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_xgb_BestGridSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_xgb_BestGridSearch),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_xgb_BestGridSearch , average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065605b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_xgb_BestGridSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba74ef",
   "metadata": {},
   "source": [
    "#### b) RandomSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b11402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 20)]\n",
    "# Learning rate - lower learning rate makes model more conservative\n",
    "learning_rate = [x for x in np.linspace(start=0.05, stop=0.5, num=10)]\n",
    "# Maximum depth of a tree - lower value will result in less overfitting\n",
    "max_depth = [int(x) for x in np.linspace(start=3, stop=10, num=5)]\n",
    "# Min child weight - higher values used to control overfitting\n",
    "min_child_weight = [2, 4, 8]\n",
    "# Gamma specifies the minimum loss reduction required to make a split \n",
    "gamma = [x for x in np.logspace(start=-3, stop=3, num = 6)]\n",
    "# Colsample_bytree \n",
    "colsample_bytree = [x for x in np.linspace(start=0.4, stop=1.0, num=8)]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_depth': max_depth,\n",
    "               'min_child_weight': min_child_weight,\n",
    "               'gamma': gamma,\n",
    "               'colsample_bytree': colsample_bytree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Random search of parameters, using 6 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, n_iter = 800, cv=cv_n, verbose=1, random_state=42, n_jobs = -1, scoring='f1_weighted')\n",
    "# Fit the random search model\n",
    "xgb_random.fit(X_train_xgb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_xgb_BestRandomSearch = xgb_random.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_xgb_BestRandomSearch),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_xgb_BestRandomSearch, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cabc3",
   "metadata": {},
   "source": [
    "#### c) Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_clf_bo(n_estimators, max_depth, min_child_weight, learning_rate, gamma, colsample_bytree):\n",
    "        \n",
    "    params_xgb = {}\n",
    "    params_xgb['n_estimators'] = round(n_estimators) # round so only tries integer values of n_estimators \n",
    "    params_xgb['max_depth'] = round(max_depth)\n",
    "    params_xgb['min_child_weight'] = round(min_child_weight)\n",
    "    params_xgb['learning_rate'] = learning_rate\n",
    "    params_xgb['gamma'] = round(gamma)\n",
    "    params_xgb['colsample_bytree'] = colsample_bytree\n",
    "        \n",
    "    scores = cross_val_score(XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42, n_jobs=-1, **params_xgb),\n",
    "                             X_train_xgb, y_train, scoring='f1_weighted', cv=cv_n).mean() # tried lots of different score metrics, yet GridSeach CV result above outperforms BO performance on test class \n",
    "    score = scores.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9009215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "params_xgb = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'max_depth': (3,7),\n",
    "    'min_child_weight': (2,8),\n",
    "    'learning_rate': (0.05, 0.5),\n",
    "    'gamma': (0.1, 1000),\n",
    "    'colsample_bytree': (0.5, 0.9)\n",
    "}\n",
    "\n",
    "xgb_bo = BayesianOptimization(xgb_clf_bo, params_xgb , random_state=42)\n",
    "xgb_bo.maximize(init_points=400, n_iter=100) # starts with 100 random points & goes for 400 iterations \n",
    "print('It takes %s seconds' % round((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_BO_xgb = xgb_bo.max['params']\n",
    "params_BO_xgb['n_estimators'] = round(params_BO_xgb['n_estimators']) \n",
    "params_BO_xgb['max_depth'] = round(params_BO_xgb['max_depth']) \n",
    "params_BO_xgb['min_child_weight'] = round(params_BO_xgb['min_child_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79756511",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_BO_best_model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=42, n_jobs=-1, **params_BO_xgb)\n",
    "xgb_BO_best_model.fit(X_train_xgb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_xgb_BestBO = xgb_BO_best_model.predict(X_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46014705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_xgb_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_train_xgb_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287da3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_xgb_BestBO = xgb_BO_best_model.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6197884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_xgb_BestBO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ec6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score: {}\".format(round(roc_auc_score(y_test, y_test_xgb_BestBO),2)))\n",
    "\n",
    "print(\"F1 Score: {}\".format(round(f1_score(y_test, y_test_xgb_BestBO, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44675234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_test_xgb_BestBO))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82961e81",
   "metadata": {},
   "source": [
    "### Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d96be9",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2182d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_features_sorted_idx = xgb_BO_best_model.feature_importances_.argsort()\n",
    "plt.barh(X.columns[xgb_features_sorted_idx], xgb_BO_best_model.feature_importances_[xgb_features_sorted_idx])\n",
    "plt.xlabel(\"XGBoost Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d9440",
   "metadata": {},
   "source": [
    "#### SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_explainer = shap.TreeExplainer(model=xgb_GridSearch_best_model, data=X_train_xgb, model_output='probability')\n",
    "xgb_shap_values = xgb_explainer(X_test_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa385dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(xgb_shap_values[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df4ef7",
   "metadata": {},
   "source": [
    "## 2.3 Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rf_RandomSearch_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model.best_params_\n",
    "best_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b78ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"PastModels/P4_T1_Full.pickle\"\n",
    "\n",
    "# save model\n",
    "pickle.dump(best_model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"PastModels/P1_T1_Full.pickle\"\n",
    "\n",
    "# load model\n",
    "loaded_model = pickle.load(open(filename, \"rb\"))\n",
    "best_model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37202c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1T1_model = pickle.load(open(\"PastModels/P1_T1_Full.pickle\", \"rb\"))\n",
    "# P1T2_model = pickle.load(open(\"PastModels/P1_T2_Full.pickle\", \"rb\"))\n",
    "# P4T1_model = pickle.load(open(\"PastModels/P4_T1_Full.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e831cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ImagesforJupyter/ROC.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f115c69",
   "metadata": {},
   "source": [
    "It is useful to plot the Reciever Operating Characteristic (ROC) curve to visualise the performance of the classifier and area under the curve, ROC AUC, has been a metric used throughout this notebook. A ROC AUC = 1 would be a perfect classifier, whereas 0.5 would be a random classifier. \n",
    "\n",
    "https://deepchecks.com/question/what-is-a-good-roc-curve-score/ provides some typical guidelines for a good ROC AUC score.\n",
    "\n",
    "Remember the true positive rate is equivalent to recall. So as we increase the false positive rate, i.e., allow more negative examples to be incorrectly classified as true, we are bound to also capture, recall, more of the true samples in general. Therefore, at the extremas where you accept no false positives, you also can't capture any true positives, however, at the other end, where you allow everything in, you will also have a true positive rate = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3803a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if type(best_model) != xgboost.sklearn.XGBClassifier:\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1_weighted = f1_score(y_test, best_model.predict(X_test), average='weighted')\n",
    "else:\n",
    "    y_pred_proba = best_model.predict_proba(X_test_xgb)[::,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1_weighted = f1_score(y_test, best_model.predict(X_test_xgb), average='weighted')\n",
    "\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"ROC AUC=\"+str(round(auc,2))+\"\\nF1_weighted=\"+str(round(f1_weighted,2)))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.title('(P4, T1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7729de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [P1T1_model, P1T2_model, P4T1_model]\n",
    "# X_data = [X_test_P1T1, X_test_P1T2, X_test_P4T1]\n",
    "# y_data = [y_test_P1T1, y_test_P1T2, y_test_P4T1]\n",
    "# sub_systems = ['(P1, T1)', '(P1, T2)', '(P4, T1)']\n",
    "\n",
    "# fpr_res, tpr_res, auc_res, f1_weighted_res = [], [], [], []\n",
    "\n",
    "# for i in range(len(models)):\n",
    "#     if type(models[i]) != xgboost.sklearn.XGBClassifier:\n",
    "#         y_pred_proba = models[i].predict_proba(X_data[i])[::,1]\n",
    "#         fpr, tpr, _ = roc_curve(y_data[i],  y_pred_proba)\n",
    "#         auc = roc_auc_score(y_data[i], y_pred_proba)\n",
    "#         f1_weighted = f1_score(y_data[i], models[i].predict(X_data[i]), average='weighted')\n",
    "#     else:\n",
    "#         print(\"Used XGBoost model\")\n",
    "#         # y_pred_proba = models[i].predict_proba(X_test_xgb)[::,1]\n",
    "#         # fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "#         # auc = roc_auc_score(y_test, y_pred_proba)\n",
    "#         # f1_weighted = f1_score(y_test, best_model.predict(X_test_xgb), average='weighted')\n",
    "#     fpr_res.append(fpr)\n",
    "#     tpr_res.append(tpr)\n",
    "#     auc_res.append(auc)\n",
    "#     f1_weighted_res.append(f1_weighted)\n",
    "\n",
    "\n",
    "# #create ROC curve\n",
    "# for i in range(len(models)):\n",
    "#     plt.plot(fpr_res[i], tpr_res[i], label=sub_systems[i]+\"\\nROC AUC=\"+str(round(auc_res[i],2))+\"\\n$F1_{weighted}$=\"+str(round(f1_weighted_res[i],2)))\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.legend(loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73694a",
   "metadata": {},
   "source": [
    "These metrics, the ROC AUC and weighted F1 score, are the ones recorded offline, to track, ideally model improvement, across time as more data is generated in the project. \n",
    "\n",
    "One notices the ROC AUC score in the figure's legend is higher than the ROC AUC score computed for the same model. The following post provides a good explanation: https://stats.stackexchange.com/questions/521503/what-is-the-reason-for-the-difference-in-auc-from-probabilities-vs-auc-from-final\n",
    "\n",
    "Essentially, here we computed the true positive and false positive rates looking at the predicted probabilities for the outputs, not just the class classifications. For e.g., if we compute the probability as 0.6 to be stable and in fact the sample is unstable, i.e., a false positive, this would be penalised more heavily in the binary classification using the `model.predict()` method vs. the `model.predict_proba()` method as utilised here. In this case we are also assigning a 0.4 probability that the model could in fact be unstable, so this point would be penalised less harshly in computing the overall ROC AUC metric. Hence, we can rationalise why the ROC AUC score in this sub-section, 2.3, is higher than that obtain in 2.2 for the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28881a",
   "metadata": {},
   "source": [
    "#### Model Interpretability and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(best_model) != xgboost.sklearn.XGBClassifier:\n",
    "    explainer = shap.TreeExplainer(model=best_model);\n",
    "    shap_values = explainer(X_test);\n",
    "    shap.plots.beeswarm(shap_values[:,:,1], max_display=10);\n",
    "else:\n",
    "    explainer = shap.TreeExplainer(model=best_model, data=X_train_xgb, model_output='probability', show=False);\n",
    "    shap_values = explainer(X_test);\n",
    "    shap.plots.beeswarm(shap_values, show=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092daa65",
   "metadata": {},
   "source": [
    "# 3. Applying the Best Model to the Candidate Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e0320",
   "metadata": {},
   "source": [
    "*N.b.* By default, I have commented out this section so I can restart the kernel & run all without interfering/re-generating important files in the DoE directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50451c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cand_df = pd.read_csv('CandidateSet.csv', index_col=[0])\n",
    "# cand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a666d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since xgboost doesn't accept the `[`, `]` in the SMILES fragments, regex operations were used to change the column names\n",
    "# the xgboost model trained on X_train_xgb, X_test_xgb with modified column names\n",
    "# thus if the best_model is xgboost, the same regex modification needs to be applied to the cand_df\n",
    "\n",
    "# if type(best_model) != xgboost.sklearn.XGBClassifier:\n",
    "#     cand_prob = best_model.predict_proba(cand_df)\n",
    "# else:\n",
    "#     cand_df.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in cand_df.columns.values]\n",
    "#     cand_prob = best_model.predict_proba(cand_df)\n",
    "    \n",
    "# cand_stability_df = pd.DataFrame(cand_prob, columns=['Unstable', 'Stable'])\n",
    "# cand_stability_df.index += 1 # R != 0 indexing like Python\n",
    "# cand_stability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this snippet is most likely not needed anymore - it looks at the tail of candidates cut-off by a top % stable criterion\n",
    "\n",
    "# top_stable_portion = 0.1 \n",
    "# sorted_cand = cand_stability_df.sort_values('Stable', ascending=False)\n",
    "# sorted_cand[:int(len(sorted_cand)*top_stable_portion)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable_prob_cutoff = 0.6\n",
    "# cand_stability_df[cand_stability_df['Stable'] > stable_prob_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable_idx = cand_stability_df[cand_stability_df['Stable'] > stable_prob_cutoff].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cand_design = pd.read_csv('CandDesign.csv', index_col=[0]) # this candidate design was converted to the candidate set in R\n",
    "# # we can filter on the candidate design from the most stable samples predicted from the candidate set - it's equivalent.\n",
    "# restricted_cand = cand_design.filter(items = stable_idx, axis=0)\n",
    "# restricted_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_x = round(len(restricted_cand)*100/len(cand_df),2)\n",
    "# print(f\"The top {top_x}% of stable candidates have been selected from the candidate set.\")\n",
    "# print(f\"This is for a minimum probability of stability of {stable_prob_cutoff}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restricted_cand.to_csv('RestrictedCandDesign.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
